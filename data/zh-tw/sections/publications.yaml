section:
  name: Publications
  id: publications
  enable: true 
  weight: 6
  showOnNavbar: true

buttons:
- name: All
  filter: "all"
- name: "NLP"
  filter: "NLP"

publications:
- title: Large Language Models on the Chessboard - A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills
  publishedIn:
    name: Symposium on Large Language Models (LLM 2023) @ IJCAI 2023
    date: 21 Aug 2023 
    url: https://bigmodel.ai/llm-ijcai23
  authors:
  - name: Mu-Tien Kuo
  - name: Chih-Chung Hsueh
  - name: Richard Tzong-Han Tsai
  paper:
    summary: While large language models have made strides in natural language processing, their proficiency in complex reasoning tasks requiring formal language comprehension, such as chess, remains less investigated. This paper probes the performance of ChatGPT, a sophisticated language model by OpenAI in tackling such complex reasoning tasks, using chess as a case study. Through robust metrics examining both the legality and quality of moves, we assess ChatGPT’s understanding of the chessboard, adherence to chess rules, and strategic decision-making abilities. Our evaluation identifies limitations within ChatGPT’s attention mechanism that affect its formal language comprehension and uncovers the model’s underdeveloped self-regulation abilities.
    url: https://arxiv.org/abs/2308.15118
  tags: ["NLP", "Complex Reasoning", "Large Language Models"]

- title: Automated Assessment of Fidelity and Interpretability - An Evaluation Framework for Large Language Models’ Explanations
  publishedIn:
    name: Proceedings of the AAAI Conference on Artificial Intelligence
    date: 24 March 2024 
    url: https://ojs.aaai.org/index.php/AAAI/article/view/30470 
  authors:
  - name: Mu-Tien Kuo
  - name: Chih-Chung Hsueh
  - name: Richard Tzong-Han Tsai
  paper:
    summary: As Large Language Models (LLMs) become more prevalent in various fields, it is crucial to rigorously assess the quality of their explanations. Our research introduces a task-agnostic framework for evaluating free-text rationales, drawing on insights from both linguistics and machine learning. We evaluate two dimensions of explainability - fidelity and interpretability. For fidelity, we propose methods suitable for proprietary LLMs where direct introspection of internal features is unattainable. For interpretability, we use language models instead of human evaluators, addressing concerns about subjectivity and scalability in evaluations.
    url: https://arxiv.org/abs/2308.15118
  tags: [ "NLP", "Large Language Models", "Explainability", "Fidelity", "Faithfulness", "Interpretability"]
